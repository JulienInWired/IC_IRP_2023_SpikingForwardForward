{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TBC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-20 23:59:25.417622: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 23:59:25.541663: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-08-20 23:59:25.566411: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-08-20 23:59:26.008457: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-08-20 23:59:26.008536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-08-20 23:59:26.008542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The directory [/home/lain/imperial2022/FFRSDTP/download/frames_number_15_split_by_number] already exists.\n",
            "The directory [/home/lain/imperial2022/FFRSDTP/download/frames_number_15_split_by_number] already exists.\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spikingjelly\n",
        "import torchvision\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "from spikingjelly.activation_based import neuron, layer, learning, surrogate, encoding, functional\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from spikingjelly.datasets.n_mnist import NMNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 指定数据集的根目录\n",
        "root = '/home/lain/imperial2022/FFRSDTP/download'\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# 创建数据集实例\n",
        "transform = Compose([\n",
        "    Lambda(lambda x: x.reshape(x.shape[0], -1))])\n",
        "train_dataset = NMNIST(root, train=True, data_type='frame', frames_number=15, split_by='number', transform=transform)\n",
        "test_dataset = NMNIST(root, train=False, data_type='frame', frames_number=15, split_by='number', transform=transform)\n",
        "train_data_loader = DataLoader(dataset=train_dataset, batch_size=50, shuffle=True, drop_last=False, num_workers=0)\n",
        "test_data_loader = DataLoader(dataset=test_dataset, batch_size=50, shuffle=True, drop_last=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0CcS-9X0azve"
      },
      "outputs": [],
      "source": [
        "class LayerOfLain():\n",
        "\n",
        "  \"\"\"\n",
        "  This class is used to instantiate the layer object in the SFF algorithm, \n",
        "  provide a training function that is uniformly called by the network during training, \n",
        "  and perform local training independently.\n",
        "\n",
        "  Member variables:\n",
        "  threshold_pos (float): used to determine whether goodness_pos is large enough and directly participate in training.\n",
        "  threshold_neg (float): used to determine whether goodness_neg is small enough and directly participate in training.\n",
        "  min_weight (float): Used to provide a hard bound when calling the STDP module.\n",
        "  max_weight (float): Used to provide a hard bound when calling the STDP module.\n",
        "  encoder (encoder): Poisson encoder, which converts traditional data into pulse shape data that conforms to Poisson distribution.\n",
        "  time_step (int): The time step length of the simulation for each data sample.\n",
        "  learning_rate (float): learning rate.\n",
        "  pre_time_au (float): Spiking neural network hyperparameters, time constants related to membrane potential decay and STDP.\n",
        "  post_time_au (float): spiking neural network hyperparameters, time constants related to membrane potential decay and STDP.\n",
        "  learner (MSTDPLearner): reward-modulated STDP learner, called when the STDP module of SFF is started.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, N_input, N_output, pre_time_au = 2.,\n",
        "               post_time_au = 100., time_step = 15,\n",
        "               batch_size = 50, learning_rate = 0.00003, threshold_both = 0.88):\n",
        "    self.single_net = nn.Sequential(\n",
        "        #layer.Flatten(start_dim=2),\n",
        "        layer.Linear(N_input, N_output, bias=False),\n",
        "        neuron.IFNode(surrogate_function=surrogate.ATan())\n",
        "    ).to(device)\n",
        "    self.threshold_pos = threshold_both\n",
        "    self.threshold_neg = threshold_both\n",
        "    self.min_weight = -1.\n",
        "    self.max_weight = 1.\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.time_step = time_step\n",
        "    self.learning_rate = learning_rate\n",
        "    self.pre_time_au = pre_time_au\n",
        "    self.post_time_au = post_time_au\n",
        "    self.batch_size = batch_size\n",
        "    self.N_output = N_output\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.learner = learning.MSTDPLearner(step_mode='s', batch_size=self.batch_size,\n",
        "                     synapse=self.single_net[0], sn=self.single_net[1],\n",
        "                     tau_pre=self.pre_time_au, tau_post=self.post_time_au,\n",
        "                     )\n",
        "    self.learner.disable()\n",
        "\n",
        "  def goodness_cal(self, output):\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    return goodness\n",
        "\n",
        "  def reward_from_goodness(self, output, pos_flag):\n",
        "    alpha_pos = 1.\n",
        "    alpha_neg = 1.\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    if(pos_flag==True):\n",
        "      return alpha_pos * (goodness - self.threshold_pos)\n",
        "    else:\n",
        "      return alpha_neg * (self.threshold_neg - goodness)\n",
        "\n",
        "\n",
        "  def forward_with_training(self, input_pos, input_neg, insight_pos, insight_neg, stdpflag = True):\n",
        "\n",
        "    weight_opter_stdp = torch.optim.SGD(self.single_net.parameters(), lr=0.01, momentum=0.)\n",
        "    weight_opter_surrogate = torch.optim.Adam(self.single_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    if(stdpflag == True):\n",
        "      with torch.no_grad():\n",
        "          self.learner.enable()\n",
        "          reward_pos = 0.\n",
        "          for t in range(self.time_step):\n",
        "              # Positive update\n",
        "              reward_pos = self.reward_from_goodness(self.single_net(input_pos[t]), True)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_pos, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "\n",
        "          reward_neg = 0.\n",
        "          for t3 in range(self.time_step):\n",
        "              # Negative update\n",
        "              reward_neg = self.reward_from_goodness(self.single_net(input_neg[t3]), False)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_neg, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "          torch.cuda.empty_cache()\n",
        "          self.learner.disable()\n",
        "      functional.reset_net(self.single_net)\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    goodness_pos = 0.\n",
        "    for t in range(self.time_step):\n",
        "        # Positive update\n",
        "        goodness_pos += self.goodness_cal(self.single_net(input_pos[t]))\n",
        "    \n",
        "    goodness_pos = goodness_pos / self.time_step\n",
        "    #print(goodness_pos)\n",
        "    goodness_neg = 0.\n",
        "    for t3 in range(self.time_step):\n",
        "        # Negative update\n",
        "        goodness_neg += self.goodness_cal(self.single_net(input_neg[t3]))\n",
        "\n",
        "    goodness_neg = goodness_neg / self.time_step\n",
        "    \n",
        "    combined_pos = self.threshold_pos - goodness_pos# - insight_pos\n",
        "    combined_neg = - self.threshold_neg + goodness_neg# - insight_neg\n",
        "\n",
        "    loss_mixed = torch.log(torch.exp(torch.cat([combined_pos, combined_neg])) + 1).mean()\n",
        "    weight_opter_surrogate.zero_grad()\n",
        "    loss_mixed.backward()\n",
        "    weight_opter_surrogate.step()\n",
        "    functional.reset_net(self.single_net)\n",
        "\n",
        "  def forward_withOUT_training(self, input_pos, input_neg):\n",
        "    total_output_pos_list = []\n",
        "    total_output_neg_list = []\n",
        "    for t2 in range(self.time_step):\n",
        "      total_output_pos_list.append((self.single_net(input_pos[t2])).detach())\n",
        "      total_output_neg_list.append((self.single_net(input_neg[t2])).detach())\n",
        "\n",
        "    total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    total_output_neg = torch.stack(total_output_neg_list, dim=0)\n",
        "    return total_output_pos, total_output_neg\n",
        "\n",
        "  def forward_withOUT_training_single(self, input_pos, firstflag):\n",
        "    total_output_pos_list = []\n",
        "    if(firstflag==0):\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    else:\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "\n",
        "\n",
        "    return total_output_pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uokJ1_8iaz3N"
      },
      "outputs": [],
      "source": [
        "def label_encoder(input, label):\n",
        "    labeled_input = input.clone()\n",
        "    for t in range(input.shape[0]):\n",
        "      start_index = 34*34\n",
        "      end_index = start_index + 10\n",
        "      labeled_input[t][:, start_index:end_index] *= 0.0\n",
        "      labeled_input[t][range(input[t].shape[0]), start_index + label] = 2 * input.max()\n",
        "      labeled_input[t][:, -34:-24] *= 0.0\n",
        "      labeled_input[t][range(input[t].shape[0]), -34+label] = 2 * input.max()\n",
        "    return labeled_input\n",
        "\n",
        "def poisson_iter(input, t):\n",
        "    batch_size, dim = input.shape\n",
        "    output = torch.zeros((t, batch_size, dim))\n",
        "    encoder = encoding.PoissonEncoder()\n",
        "    for i in range(t):\n",
        "        encoden_input = encoder(input)\n",
        "        output[i] = encoden_input\n",
        "    return output\n",
        "\n",
        "class NetOfLain(torch.nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    This class is used to instantiate the net object in the SFF algorithm, coordinate and call the training functions of each layer during training, so that they can perform local training independently.\n",
        "\n",
        "    Member variables:\n",
        "    lain_layers (LayerOfLain list): used to store layers for constructing SFF spiking neural network.\n",
        "    insight_pos (float): The key constant for SFF to realize layer collaboration, which is the sum of the goodness of each layer after positive data propagation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, lain_dimension):\n",
        "        super().__init__()\n",
        "        self.lain_layers = []\n",
        "        self.insight_pos = 0.\n",
        "        self.insight_neg = 0.\n",
        "        for d in range(len(lain_dimension) - 1):\n",
        "            if(d == 0):\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100.)\n",
        "              self.lain_layers.append(layer)\n",
        "            else:\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100., learning_rate=0.0001, threshold_both=0.9)\n",
        "              self.lain_layers.append(layer)\n",
        "\n",
        "    def network_train_layers(self, train_data_loader, epo):\n",
        "      torch.cuda.empty_cache()\n",
        "      for i, lain_layer in enumerate(self.lain_layers):\n",
        "        print('training layer', i, '...')\n",
        "        for features, labels in tqdm(train_data_loader):\n",
        "          if(epo > i*1):\n",
        "            break\n",
        "          torch.cuda.empty_cache()\n",
        "          features, labels = features.to(device), labels.to(device)\n",
        "          features = features.transpose(0, 1)\n",
        "          features_pos = label_encoder(features, labels)\n",
        "          rnd = torch.randperm(features.size(1))\n",
        "          features_neg = label_encoder(features, labels[rnd])\n",
        "          #features_pos = poisson_iter(features_pos, lain_layer.time_step)\n",
        "          features_pos = features_pos.to(device)\n",
        "          #features_neg = poisson_iter(features_neg, lain_layer.time_step)\n",
        "          features_neg = features_neg.to(device)\n",
        "          del features, labels\n",
        "          torch.cuda.empty_cache()\n",
        "          #features_pos = features_pos.transpose(0, 1)\n",
        "          #features_neg = features_neg.transpose(0, 1)\n",
        "          #self.insight_pos = self.network_collaboration(features_pos)\n",
        "          #self.insight_neg = self.network_collaboration(features_neg)\n",
        "          positive_hidden, negative_hidden = features_pos, features_neg\n",
        "          if(i > 0) :\n",
        "            for o in range(i):\n",
        "              positive_hidden, negative_hidden = self.lain_layers[o].forward_withOUT_training(positive_hidden, negative_hidden)\n",
        "              positive_hidden = positive_hidden*10\n",
        "              negative_hidden = negative_hidden*10\n",
        "              functional.reset_net(self.lain_layers[o].single_net)\n",
        "          torch.cuda.empty_cache()\n",
        "          if(i==0):\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=False)\n",
        "          else:\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=True)\n",
        "\n",
        "    def network_predict(self, input):\n",
        "      every_labels_goodness = []\n",
        "      for label in range(10):\n",
        "        hidden = label_encoder(input, label)\n",
        "        #hidden = poisson_iter(hidden, 50)\n",
        "        hidden = hidden.to(device)\n",
        "        torch.cuda.empty_cache()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "        #print(every_layer_goodness[0])\n",
        "        every_labels_goodness += [sum(every_layer_goodness).unsqueeze(1)]\n",
        "        del hidden\n",
        "        #for lain_layer in self.lain_layers:\n",
        "          #functional.reset_net(lain_layer.single_net)\n",
        "        torch.cuda.empty_cache()\n",
        "      every_labels_goodness = torch.cat(every_labels_goodness, 1)\n",
        "      #print(every_labels_goodness.argmax(1))\n",
        "      return every_labels_goodness.argmax(1)\n",
        "\n",
        "    def network_collaboration(self, input):\n",
        "        hidden = input.clone()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "        del hidden\n",
        "        torch.cuda.empty_cache()\n",
        "        return sum(every_layer_goodness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV0VhHBTbYIw",
        "outputId": "f123b1f9-a8a2-4741-bddf-2557efbfca90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "training layer 0 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [01:34<00:00, 12.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training layer 1 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [04:54<00:00,  4.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test error: 0.4156000143289566\n",
            "Epoch: 1\n",
            "training layer 0 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1200 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training layer 1 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [04:53<00:00,  4.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test error: 0.41490001454949377\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(1000)\n",
        "    torch.cuda.empty_cache()\n",
        "    alice = NetOfLain([2312, 1000, 500])\n",
        "    for epo in range(2):\n",
        "      print(\"Epoch:\", epo)\n",
        "      torch.cuda.empty_cache()\n",
        "      alice.network_train_layers(train_data_loader, epo)\n",
        "      countT = 0.\n",
        "      lossT = 0.\n",
        "      for test_x, test_y in test_data_loader:\n",
        "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
        "        test_x = test_x.transpose(0, 1)\n",
        "        lossT += 1.0 - alice.network_predict(test_x).eq(test_y).float().mean().item()\n",
        "        countT += 1\n",
        "        for lain_layer in alice.lain_layers:\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "      print('test error:', lossT / countT)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
