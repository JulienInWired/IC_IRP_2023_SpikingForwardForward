{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install spikingjelly\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spikingjelly\n",
        "import torchvision\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "from spikingjelly.activation_based import neuron, layer, learning, surrogate, encoding, functional\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "transform2 = Compose([\n",
        "    ToTensor(),\n",
        "    #Normalize((0.1307,), (0.3081,)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "data_dir = './data'\n",
        "device = 'cuda:0'\n",
        "b = 500\n",
        "j = 2\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=data_dir,\n",
        "    train=True,\n",
        "    transform=transform2,\n",
        "    download=True\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=data_dir,\n",
        "    train=False,\n",
        "    transform=transform2,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "train_data_loader = data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=b,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=j,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_data_loader = data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=b,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=j,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "IIIiis9kazdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793c0b4a-984f-4757-da99-c43969ac6a7b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spikingjelly in /usr/local/lib/python3.10/dist-packages (0.0.0.0.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (2.0.1+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (4.66.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (0.15.2+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->spikingjelly) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->spikingjelly) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->spikingjelly) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->spikingjelly) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->spikingjelly) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerOfLain():\n",
        "  def __init__(self, N_input, N_output, pre_time_au = 2.,\n",
        "               post_time_au = 100., time_step = 50,\n",
        "               batch_size = 500, learning_rate = 0.0003, threshold_both = 0.05):\n",
        "    self.single_net = nn.Sequential(\n",
        "        layer.Linear(N_input, N_output, bias=False),\n",
        "        neuron.IFNode(surrogate_function=surrogate.ATan())\n",
        "    ).to(device)\n",
        "    self.threshold_pos = threshold_both\n",
        "    self.threshold_neg = threshold_both\n",
        "    self.min_weight = -1.\n",
        "    self.max_weight = 1.\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.time_step = time_step\n",
        "    self.learning_rate = learning_rate\n",
        "    self.pre_time_au = pre_time_au\n",
        "    self.post_time_au = post_time_au\n",
        "    self.batch_size = batch_size\n",
        "    self.N_output = N_output\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.learner = learning.MSTDPLearner(step_mode='s', batch_size=self.batch_size,\n",
        "                     synapse=self.single_net[0], sn=self.single_net[1],\n",
        "                     tau_pre=self.pre_time_au, tau_post=self.post_time_au,\n",
        "                     )\n",
        "    self.learner.disable()\n",
        "\n",
        "  def goodness_cal(self, output):\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    #print(goodness)\n",
        "    return goodness\n",
        "\n",
        "  def reward_from_goodness(self, output, pos_flag):\n",
        "    alpha_pos = 1.\n",
        "    alpha_neg = 1.\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    if(pos_flag==True):\n",
        "      return alpha_pos * (goodness - self.threshold_pos)\n",
        "    else:\n",
        "      return alpha_neg * (self.threshold_neg - goodness)\n",
        "\n",
        "\n",
        "  def forward_with_training(self, input_pos, input_neg, insight_pos, insight_neg, stdpflag = True):\n",
        "\n",
        "    weight_opter_stdp = torch.optim.SGD(self.single_net.parameters(), lr=0.01, momentum=0.)\n",
        "    weight_opter_surrogate = torch.optim.Adam(self.single_net.parameters(), lr=self.learning_rate)\n",
        "    if(stdpflag == True):\n",
        "      with torch.no_grad():\n",
        "          self.learner.enable()\n",
        "          reward_pos = 0.\n",
        "          for t in range(self.time_step):\n",
        "              # Positive update\n",
        "              reward_pos = self.reward_from_goodness(self.single_net(input_pos[t]), True)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_pos, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "\n",
        "          reward_neg = 0.\n",
        "          for t3 in range(self.time_step):\n",
        "              # Negative update\n",
        "              reward_neg = self.reward_from_goodness(self.single_net(input_neg[t3]), False)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_neg, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "          torch.cuda.empty_cache()\n",
        "          self.learner.disable()\n",
        "      functional.reset_net(self.single_net)\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    goodness_pos = 0.\n",
        "    for t in range(self.time_step):\n",
        "        # Positive update\n",
        "        #print(input_pos.max())\n",
        "        goodness_pos += self.goodness_cal(self.single_net(input_pos[t]))\n",
        "\n",
        "    goodness_pos = goodness_pos / self.time_step\n",
        "\n",
        "    goodness_neg = 0.\n",
        "    for t3 in range(self.time_step):\n",
        "        # Negative update\n",
        "        goodness_neg += self.goodness_cal(self.single_net(input_neg[t3]))\n",
        "\n",
        "    goodness_neg = goodness_neg / self.time_step\n",
        "\n",
        "    combined_pos = self.threshold_pos - goodness_pos - insight_pos\n",
        "    combined_neg = - self.threshold_neg + goodness_neg - insight_neg\n",
        "\n",
        "    loss_mixed = torch.log(torch.exp(torch.cat([combined_pos, combined_neg])) + 1).mean()\n",
        "    weight_opter_surrogate.zero_grad()\n",
        "    loss_mixed.backward()\n",
        "    weight_opter_surrogate.step()\n",
        "    functional.reset_net(self.single_net)\n",
        "\n",
        "  def forward_withOUT_training(self, input_pos, input_neg):\n",
        "    total_output_pos_list = []\n",
        "    total_output_neg_list = []\n",
        "    for t2 in range(self.time_step):\n",
        "      total_output_pos_list.append((self.single_net(input_pos[t2])).detach())\n",
        "      total_output_neg_list.append((self.single_net(input_neg[t2])).detach())\n",
        "\n",
        "    total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    total_output_neg = torch.stack(total_output_neg_list, dim=0)\n",
        "    return total_output_pos, total_output_neg\n",
        "\n",
        "  def forward_withOUT_training_single(self, input_pos, firstflag):\n",
        "    total_output_pos_list = []\n",
        "    if(firstflag==0):\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    else:\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "\n",
        "\n",
        "    return total_output_pos"
      ],
      "metadata": {
        "id": "0CcS-9X0azve"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoder(input, label):\n",
        "    labeled_input = input.clone()\n",
        "    labeled_input[:, :10] *= 0.0\n",
        "    labeled_input[range(input.shape[0]), label] = 1.0\n",
        "    labeled_input[:, -28:-18] *= 0.0\n",
        "    labeled_input[range(input.shape[0]), -28+label] = 1.0\n",
        "    return labeled_input\n",
        "\n",
        "def poisson_iter(input, t):\n",
        "    batch_size, dim = input.shape\n",
        "    output = torch.zeros((t, batch_size, dim))\n",
        "    encoder = encoding.PoissonEncoder()\n",
        "    for i in range(t):\n",
        "        encoden_input = encoder(input)\n",
        "        output[i] = encoden_input\n",
        "    return output\n",
        "\n",
        "class NetOfLain(torch.nn.Module):\n",
        "    def __init__(self, lain_dimension):\n",
        "        super().__init__()\n",
        "        self.lain_layers = []\n",
        "        self.insight_pos = 0.\n",
        "        self.insight_neg = 0.\n",
        "        for d in range(len(lain_dimension) - 1):\n",
        "            if(d == 0):\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100.)\n",
        "              self.lain_layers.append(layer)\n",
        "            else:\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100., learning_rate = 0.004, threshold_both=0.04)\n",
        "              self.lain_layers.append(layer)\n",
        "\n",
        "    def network_train_layers(self, train_data_loader, epo):\n",
        "      torch.cuda.empty_cache()\n",
        "      for i, lain_layer in enumerate(self.lain_layers):\n",
        "        print('training layer', i, '...')\n",
        "        for features, labels in tqdm(train_data_loader):\n",
        "          if(epo > i*1):\n",
        "            break\n",
        "          torch.cuda.empty_cache()\n",
        "          features, labels = features.to(device), labels.to(device)\n",
        "          features_pos = label_encoder(features, labels)\n",
        "          rnd = torch.randperm(features.size(0))\n",
        "          features_neg = label_encoder(features, labels[rnd])\n",
        "          features_pos = poisson_iter(features_pos, lain_layer.time_step)\n",
        "          features_pos = features_pos.to(device)\n",
        "          features_neg = poisson_iter(features_neg, lain_layer.time_step)\n",
        "          features_neg = features_neg.to(device)\n",
        "          del features, labels\n",
        "          torch.cuda.empty_cache()\n",
        "          #features_pos = features_pos.transpose(0, 1)\n",
        "          #features_neg = features_neg.transpose(0, 1)\n",
        "          self.insight_pos = self.network_collaboration(features_pos)\n",
        "          self.insight_neg = self.network_collaboration(features_neg)\n",
        "          positive_hidden, negative_hidden = features_pos, features_neg\n",
        "          if(i > 0) :\n",
        "            for o in range(i):\n",
        "              positive_hidden, negative_hidden = self.lain_layers[o].forward_withOUT_training(positive_hidden, negative_hidden)\n",
        "              functional.reset_net(self.lain_layers[o].single_net)\n",
        "          torch.cuda.empty_cache()\n",
        "          if(i==0):\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=False)\n",
        "          else:\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=True)\n",
        "\n",
        "    def network_predict(self, input):\n",
        "      every_labels_goodness = []\n",
        "      for label in range(10):\n",
        "        hidden = label_encoder(input, label)\n",
        "        hidden = poisson_iter(hidden, 50)\n",
        "        hidden = hidden.to(device)\n",
        "        torch.cuda.empty_cache()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "        every_labels_goodness += [sum(every_layer_goodness).unsqueeze(1)]\n",
        "        del hidden\n",
        "        #for lain_layer in self.lain_layers:\n",
        "          #functional.reset_net(lain_layer.single_net)\n",
        "        torch.cuda.empty_cache()\n",
        "      every_labels_goodness = torch.cat(every_labels_goodness, 1)\n",
        "      return every_labels_goodness.argmax(1)\n",
        "\n",
        "    def network_collaboration(self, input):\n",
        "        hidden = input.clone()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "        del hidden\n",
        "        torch.cuda.empty_cache()\n",
        "        return sum(every_layer_goodness)"
      ],
      "metadata": {
        "id": "uokJ1_8iaz3N"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(1000)\n",
        "    torch.cuda.empty_cache()\n",
        "    alice = NetOfLain([784, 500, 500])\n",
        "    for epo in range(1):\n",
        "      print(\"Epoch:\", epo)\n",
        "      torch.cuda.empty_cache()\n",
        "      alice.network_train_layers(train_data_loader, epo)\n",
        "      countT = 0.\n",
        "      lossT = 0.\n",
        "      for test_x, test_y in test_data_loader:\n",
        "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
        "        lossT += 1.0 - alice.network_predict(test_x).eq(test_y).float().mean().item()\n",
        "        countT += 1\n",
        "        for lain_layer in alice.lain_layers:\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "      print('test error:', lossT / countT)"
      ],
      "metadata": {
        "id": "fV0VhHBTbYIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ded6878-6132-47b8-b324-3dc25893cb1d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "training layer 0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [01:07<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training layer 1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [02:50<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test error: 0.09949995875358582\n"
          ]
        }
      ]
    }
  ]
}