{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install spikingjelly\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spikingjelly\n",
        "import torchvision\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "from spikingjelly.activation_based import neuron, layer, learning, surrogate, encoding, functional\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "transform2 = Compose([\n",
        "    ToTensor(),\n",
        "    #Normalize((0.1307,), (0.3081,)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "data_dir = './data'\n",
        "device = 'cuda:0'\n",
        "b = 500\n",
        "j = 2\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=data_dir,\n",
        "    train=True,\n",
        "    transform=transform2,\n",
        "    download=True\n",
        ")\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=data_dir,\n",
        "    train=False,\n",
        "    transform=transform2,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "train_data_loader = data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=b,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=j,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_data_loader = data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=b,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=j,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "IIIiis9kazdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a389239c-e0dd-4970-bf33-3828f2fb2ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spikingjelly in /usr/local/lib/python3.10/dist-packages (0.0.0.0.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (2.0.1+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (4.66.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (0.15.2+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->spikingjelly) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->spikingjelly) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->spikingjelly) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->spikingjelly) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->spikingjelly) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerOfLain():\n",
        "\n",
        "  \"\"\"\n",
        "  This class is used to instantiate the layer object in the SFF algorithm,\n",
        "  provide a training function that is uniformly called by the network during training,\n",
        "  and perform local training independently.\n",
        "\n",
        "  Member variables:\n",
        "  threshold_pos (float): used to determine whether goodness_pos is large enough and directly participate in training.\n",
        "  threshold_neg (float): used to determine whether goodness_neg is small enough and directly participate in training.\n",
        "  min_weight (float): Used to provide a hard bound when calling the STDP module.\n",
        "  max_weight (float): Used to provide a hard bound when calling the STDP module.\n",
        "  encoder (encoder): Poisson encoder, which converts traditional data into pulse shape data that conforms to Poisson distribution.\n",
        "  time_step (int): The time step length of the simulation for each data sample.\n",
        "  learning_rate (float): learning rate.\n",
        "  pre_time_au (float): Spiking neural network hyperparameters, time constants related to membrane potential decay and STDP.\n",
        "  post_time_au (float): spiking neural network hyperparameters, time constants related to membrane potential decay and STDP.\n",
        "  learner (MSTDPLearner): reward-modulated STDP learner, called when the STDP module of SFF is started.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, N_input, N_output, pre_time_au = 2.,\n",
        "               post_time_au = 100., time_step = 50,\n",
        "               batch_size = 500, learning_rate = 0.0003, threshold_both = 0.06):\n",
        "    self.single_net = nn.Sequential(\n",
        "        layer.Linear(N_input, N_output, bias=False),\n",
        "        neuron.IFNode(surrogate_function=surrogate.ATan())\n",
        "    ).to(device)\n",
        "    self.threshold_pos = threshold_both\n",
        "    self.threshold_neg = threshold_both\n",
        "    self.min_weight = -1.\n",
        "    self.max_weight = 1.\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.time_step = time_step\n",
        "    self.learning_rate = learning_rate\n",
        "    self.pre_time_au = pre_time_au\n",
        "    self.post_time_au = post_time_au\n",
        "    self.batch_size = batch_size\n",
        "    self.N_output = N_output\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.learner = learning.MSTDPLearner(step_mode='s', batch_size=self.batch_size,\n",
        "                     synapse=self.single_net[0], sn=self.single_net[1],\n",
        "                     tau_pre=self.pre_time_au, tau_post=self.post_time_au,\n",
        "                     )\n",
        "    self.learner.disable()\n",
        "\n",
        "  def goodness_cal(self, output):\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    #print(goodness)\n",
        "    return goodness\n",
        "\n",
        "  def reward_from_goodness(self, output, pos_flag):\n",
        "    alpha_pos = 1.\n",
        "    alpha_neg = 1.\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    if(pos_flag==True):\n",
        "      return alpha_pos * (goodness - self.threshold_pos)\n",
        "    else:\n",
        "      return alpha_neg * (self.threshold_neg - goodness)\n",
        "\n",
        "\n",
        "  def forward_with_training(self, input_pos, input_neg, insight_pos, insight_neg, stdpflag = True):\n",
        "\n",
        "    weight_opter_stdp = torch.optim.SGD(self.single_net.parameters(), lr=0.01, momentum=0.)\n",
        "    weight_opter_surrogate = torch.optim.Adam(self.single_net.parameters(), lr=self.learning_rate)\n",
        "    if(stdpflag == True):\n",
        "      with torch.no_grad():\n",
        "          self.learner.enable()\n",
        "          reward_pos = 0.\n",
        "          for t in range(self.time_step):\n",
        "              # Positive update\n",
        "              reward_pos = self.reward_from_goodness(self.single_net(input_pos[t]), True)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_pos, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "\n",
        "          reward_neg = 0.\n",
        "          for t3 in range(self.time_step):\n",
        "              # Negative update\n",
        "              reward_neg = self.reward_from_goodness(self.single_net(input_neg[t3]), False)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_neg, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "          torch.cuda.empty_cache()\n",
        "          self.learner.disable()\n",
        "      functional.reset_net(self.single_net)\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    goodness_pos = 0.\n",
        "    for t in range(self.time_step):\n",
        "        # Positive update\n",
        "        #print(input_pos.max())\n",
        "        goodness_pos += self.goodness_cal(self.single_net(input_pos[t]))\n",
        "\n",
        "    goodness_pos = goodness_pos / self.time_step\n",
        "\n",
        "    goodness_neg = 0.\n",
        "    for t3 in range(self.time_step):\n",
        "        # Negative update\n",
        "        goodness_neg += self.goodness_cal(self.single_net(input_neg[t3]))\n",
        "\n",
        "    goodness_neg = goodness_neg / self.time_step\n",
        "\n",
        "    combined_pos = self.threshold_pos - goodness_pos - insight_pos\n",
        "    combined_neg = - self.threshold_neg + goodness_neg - insight_neg\n",
        "\n",
        "    loss_mixed = torch.log(torch.exp(torch.cat([combined_pos, combined_neg])) + 1).mean()\n",
        "    weight_opter_surrogate.zero_grad()\n",
        "    loss_mixed.backward()\n",
        "    weight_opter_surrogate.step()\n",
        "    functional.reset_net(self.single_net)\n",
        "\n",
        "  def forward_withOUT_training(self, input_pos, input_neg):\n",
        "    total_output_pos_list = []\n",
        "    total_output_neg_list = []\n",
        "    for t2 in range(self.time_step):\n",
        "      total_output_pos_list.append((self.single_net(input_pos[t2])).detach())\n",
        "      total_output_neg_list.append((self.single_net(input_neg[t2])).detach())\n",
        "\n",
        "    total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    total_output_neg = torch.stack(total_output_neg_list, dim=0)\n",
        "    return total_output_pos, total_output_neg\n",
        "\n",
        "  def forward_withOUT_training_single(self, input_pos, firstflag):\n",
        "    total_output_pos_list = []\n",
        "    if(firstflag==0):\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    else:\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "\n",
        "\n",
        "    return total_output_pos"
      ],
      "metadata": {
        "id": "0CcS-9X0azve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoder(input, label):\n",
        "    labeled_input = input.clone()\n",
        "    labeled_input[:, :10] *= 0.0\n",
        "    labeled_input[range(input.shape[0]), label] = 1.0\n",
        "    labeled_input[:, -28:-18] *= 0.0\n",
        "    labeled_input[range(input.shape[0]), -28+label] = 1.0\n",
        "    return labeled_input\n",
        "\n",
        "def poisson_iter(input, t):\n",
        "    batch_size, dim = input.shape\n",
        "    output = torch.zeros((t, batch_size, dim))\n",
        "    encoder = encoding.PoissonEncoder()\n",
        "    for i in range(t):\n",
        "        encoden_input = encoder(input)\n",
        "        output[i] = encoden_input\n",
        "    return output\n",
        "\n",
        "class NetOfLain(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    This class is used to instantiate the net object in the SFF algorithm, coordinate and call the training functions of each layer during training, so that they can perform local training independently.\n",
        "\n",
        "    Member variables:\n",
        "    lain_layers (LayerOfLain list): used to store layers for constructing SFF spiking neural network.\n",
        "    insight_pos (float): The key constant for SFF to realize layer collaboration, which is the sum of the goodness of each layer after positive data propagation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lain_dimension):\n",
        "        super().__init__()\n",
        "        self.lain_layers = []\n",
        "        self.insight_pos = 0.\n",
        "        self.insight_neg = 0.\n",
        "        for d in range(len(lain_dimension) - 1):\n",
        "            if(d == 0):\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100.)\n",
        "              self.lain_layers.append(layer)\n",
        "            else:\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100., learning_rate = 0.004, threshold_both=0.04)\n",
        "              self.lain_layers.append(layer)\n",
        "\n",
        "    def network_train_layers(self, train_data_loader, epo):\n",
        "      torch.cuda.empty_cache()\n",
        "      for i, lain_layer in enumerate(self.lain_layers):\n",
        "        print('training layer', i, '...')\n",
        "        for features, labels in tqdm(train_data_loader):\n",
        "          #if(epo > i*1):\n",
        "            #break\n",
        "          torch.cuda.empty_cache()\n",
        "          features, labels = features.to(device), labels.to(device)\n",
        "          features_pos = label_encoder(features, labels)\n",
        "          rnd = torch.randperm(features.size(0))\n",
        "          features_neg = label_encoder(features, labels[rnd])\n",
        "          features_pos = poisson_iter(features_pos, lain_layer.time_step)\n",
        "          features_pos = features_pos.to(device)\n",
        "          features_neg = poisson_iter(features_neg, lain_layer.time_step)\n",
        "          features_neg = features_neg.to(device)\n",
        "          del features, labels\n",
        "          torch.cuda.empty_cache()\n",
        "          #features_pos = features_pos.transpose(0, 1)\n",
        "          #features_neg = features_neg.transpose(0, 1)\n",
        "          self.insight_pos = self.network_collaboration(features_pos)\n",
        "          self.insight_neg = self.network_collaboration(features_neg)\n",
        "          positive_hidden, negative_hidden = features_pos, features_neg\n",
        "          if(i > 0) :\n",
        "            for o in range(i):\n",
        "              positive_hidden, negative_hidden = self.lain_layers[o].forward_withOUT_training(positive_hidden, negative_hidden)\n",
        "              functional.reset_net(self.lain_layers[o].single_net)\n",
        "          torch.cuda.empty_cache()\n",
        "          if(i==0):\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=False)\n",
        "          else:\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=False)\n",
        "\n",
        "    def network_predict(self, input):\n",
        "      every_labels_goodness = []\n",
        "      for label in range(10):\n",
        "        hidden = label_encoder(input, label)\n",
        "        hidden = poisson_iter(hidden, 50)\n",
        "        hidden = hidden.to(device)\n",
        "        torch.cuda.empty_cache()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "        every_labels_goodness += [sum(every_layer_goodness).unsqueeze(1)]\n",
        "        del hidden\n",
        "        for lain_layer in self.lain_layers:\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "        torch.cuda.empty_cache()\n",
        "      every_labels_goodness = torch.cat(every_labels_goodness, 1)\n",
        "      return every_labels_goodness.argmax(1)\n",
        "\n",
        "    def network_collaboration(self, input):\n",
        "        hidden = input.clone()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "        del hidden\n",
        "        torch.cuda.empty_cache()\n",
        "        return sum(every_layer_goodness)"
      ],
      "metadata": {
        "id": "uokJ1_8iaz3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(1000)\n",
        "    torch.cuda.empty_cache()\n",
        "    alice = NetOfLain([784, 1000, 400, 200])\n",
        "    for epo in range(1):\n",
        "      print(\"Epoch:\", epo)\n",
        "      torch.cuda.empty_cache()\n",
        "      alice.network_train_layers(train_data_loader, epo)\n",
        "      countT = 0.\n",
        "      lossT = 0.\n",
        "      for test_x, test_y in test_data_loader:\n",
        "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
        "        lossT += 1.0 - alice.network_predict(test_x).eq(test_y).float().mean().item()\n",
        "        countT += 1\n",
        "        for lain_layer in alice.lain_layers:\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "      print('test error:', lossT / countT)"
      ],
      "metadata": {
        "id": "fV0VhHBTbYIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb7afb5-dd1b-4059-f8b9-07289ce49dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "training layer 0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [01:12<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training layer 1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [01:16<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training layer 2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [01:23<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test error: 0.22699996531009675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Spiking Forward-Forward trained spiking neural network that does not use the layer collaboration strategy is added below.\n",
        "\n",
        "## It can be found that the positive effect of using layer collaboration strategy on the model is significant."
      ],
      "metadata": {
        "id": "dEtm_HouM4fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install spikingjelly\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spikingjelly\n",
        "import torchvision\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "from spikingjelly.activation_based import neuron, layer, learning, surrogate, encoding, functional\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "transform2 = Compose([\n",
        "    ToTensor(),\n",
        "    #Normalize((0.1307,), (0.3081,)),\n",
        "    Lambda(lambda x: torch.flatten(x))])\n",
        "data_dir = './data'\n",
        "device = 'cuda:0'\n",
        "b = 500\n",
        "j = 2\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=data_dir,\n",
        "    train=True,\n",
        "    transform=transform2,\n",
        "    download=True\n",
        ")\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=data_dir,\n",
        "    train=False,\n",
        "    transform=transform2,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "train_data_loader = data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=b,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=j,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_data_loader = data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=b,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        "    num_workers=j,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "class LayerOfLain():\n",
        "\n",
        "  \"\"\"\n",
        "  This class is used to instantiate the layer object in the SFF algorithm,\n",
        "  provide a training function that is uniformly called by the network during training,\n",
        "  and perform local training independently.\n",
        "\n",
        "  Member variables:\n",
        "  threshold_pos (float): used to determine whether goodness_pos is large enough and directly participate in training.\n",
        "  threshold_neg (float): used to determine whether goodness_neg is small enough and directly participate in training.\n",
        "  min_weight (float): Used to provide a hard bound when calling the STDP module.\n",
        "  max_weight (float): Used to provide a hard bound when calling the STDP module.\n",
        "  encoder (encoder): Poisson encoder, which converts traditional data into pulse shape data that conforms to Poisson distribution.\n",
        "  time_step (int): The time step length of the simulation for each data sample.\n",
        "  learning_rate (float): learning rate.\n",
        "  pre_time_au (float): Spiking neural network hyperparameters, time constants related to membrane potential decay and STDP.\n",
        "  post_time_au (float): spiking neural network hyperparameters, time constants related to membrane potential decay and STDP.\n",
        "  learner (MSTDPLearner): reward-modulated STDP learner, called when the STDP module of SFF is started.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, N_input, N_output, pre_time_au = 2.,\n",
        "               post_time_au = 100., time_step = 50,\n",
        "               batch_size = 500, learning_rate = 0.0003, threshold_both = 0.06):\n",
        "    self.single_net = nn.Sequential(\n",
        "        layer.Linear(N_input, N_output, bias=False),\n",
        "        neuron.IFNode(surrogate_function=surrogate.ATan())\n",
        "    ).to(device)\n",
        "    self.threshold_pos = threshold_both\n",
        "    self.threshold_neg = threshold_both\n",
        "    self.min_weight = -1.\n",
        "    self.max_weight = 1.\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.time_step = time_step\n",
        "    self.learning_rate = learning_rate\n",
        "    self.pre_time_au = pre_time_au\n",
        "    self.post_time_au = post_time_au\n",
        "    self.batch_size = batch_size\n",
        "    self.N_output = N_output\n",
        "    self.encoder = encoding.PoissonEncoder()\n",
        "    self.learner = learning.MSTDPLearner(step_mode='s', batch_size=self.batch_size,\n",
        "                     synapse=self.single_net[0], sn=self.single_net[1],\n",
        "                     tau_pre=self.pre_time_au, tau_post=self.post_time_au,\n",
        "                     )\n",
        "    self.learner.disable()\n",
        "\n",
        "  def goodness_cal(self, output):\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    #print(goodness)\n",
        "    return goodness\n",
        "\n",
        "  def reward_from_goodness(self, output, pos_flag):\n",
        "    alpha_pos = 1.\n",
        "    alpha_neg = 1.\n",
        "    goodness = output.pow(2).mean(1)\n",
        "    if(pos_flag==True):\n",
        "      return alpha_pos * (goodness - self.threshold_pos)\n",
        "    else:\n",
        "      return alpha_neg * (self.threshold_neg - goodness)\n",
        "\n",
        "\n",
        "  def forward_with_training(self, input_pos, input_neg, insight_pos, insight_neg, stdpflag = True):\n",
        "\n",
        "    weight_opter_stdp = torch.optim.SGD(self.single_net.parameters(), lr=0.01, momentum=0.)\n",
        "    weight_opter_surrogate = torch.optim.Adam(self.single_net.parameters(), lr=self.learning_rate)\n",
        "    if(stdpflag == True):\n",
        "      with torch.no_grad():\n",
        "          self.learner.enable()\n",
        "          reward_pos = 0.\n",
        "          for t in range(self.time_step):\n",
        "              # Positive update\n",
        "              reward_pos = self.reward_from_goodness(self.single_net(input_pos[t]), True)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_pos, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "\n",
        "          reward_neg = 0.\n",
        "          for t3 in range(self.time_step):\n",
        "              # Negative update\n",
        "              reward_neg = self.reward_from_goodness(self.single_net(input_neg[t3]), False)\n",
        "\n",
        "              weight_opter_stdp.zero_grad()\n",
        "              self.learner.step(reward_neg, on_grad=True)\n",
        "              weight_opter_stdp.step()\n",
        "          self.learner.reset()\n",
        "          torch.cuda.empty_cache()\n",
        "          self.learner.disable()\n",
        "      functional.reset_net(self.single_net)\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    goodness_pos = 0.\n",
        "    for t in range(self.time_step):\n",
        "        # Positive update\n",
        "        #print(input_pos.max())\n",
        "        goodness_pos += self.goodness_cal(self.single_net(input_pos[t]))\n",
        "\n",
        "    goodness_pos = goodness_pos / self.time_step\n",
        "\n",
        "    goodness_neg = 0.\n",
        "    for t3 in range(self.time_step):\n",
        "        # Negative update\n",
        "        goodness_neg += self.goodness_cal(self.single_net(input_neg[t3]))\n",
        "\n",
        "    goodness_neg = goodness_neg / self.time_step\n",
        "\n",
        "    combined_pos = self.threshold_pos - goodness_pos# - insight_pos\n",
        "    combined_neg = - self.threshold_neg + goodness_neg# - insight_neg\n",
        "\n",
        "    loss_mixed = torch.log(torch.exp(torch.cat([combined_pos, combined_neg])) + 1).mean()\n",
        "    weight_opter_surrogate.zero_grad()\n",
        "    loss_mixed.backward()\n",
        "    weight_opter_surrogate.step()\n",
        "    functional.reset_net(self.single_net)\n",
        "\n",
        "  def forward_withOUT_training(self, input_pos, input_neg):\n",
        "    total_output_pos_list = []\n",
        "    total_output_neg_list = []\n",
        "    for t2 in range(self.time_step):\n",
        "      total_output_pos_list.append((self.single_net(input_pos[t2])).detach())\n",
        "      total_output_neg_list.append((self.single_net(input_neg[t2])).detach())\n",
        "\n",
        "    total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    total_output_neg = torch.stack(total_output_neg_list, dim=0)\n",
        "    return total_output_pos, total_output_neg\n",
        "\n",
        "  def forward_withOUT_training_single(self, input_pos, firstflag):\n",
        "    total_output_pos_list = []\n",
        "    if(firstflag==0):\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "    else:\n",
        "      for t2 in range(self.time_step):\n",
        "        total_output_pos_list.append(self.single_net(input_pos[t2]).detach())\n",
        "      total_output_pos = torch.stack(total_output_pos_list, dim=0)\n",
        "\n",
        "\n",
        "    return total_output_pos\n",
        "\n",
        "def label_encoder(input, label):\n",
        "    labeled_input = input.clone()\n",
        "    labeled_input[:, :10] *= 0.0\n",
        "    labeled_input[range(input.shape[0]), label] = 1.0\n",
        "    labeled_input[:, -28:-18] *= 0.0\n",
        "    labeled_input[range(input.shape[0]), -28+label] = 1.0\n",
        "    return labeled_input\n",
        "\n",
        "def poisson_iter(input, t):\n",
        "    batch_size, dim = input.shape\n",
        "    output = torch.zeros((t, batch_size, dim))\n",
        "    encoder = encoding.PoissonEncoder()\n",
        "    for i in range(t):\n",
        "        encoden_input = encoder(input)\n",
        "        output[i] = encoden_input\n",
        "    return output\n",
        "\n",
        "class NetOfLain(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    This class is used to instantiate the net object in the SFF algorithm, coordinate and call the training functions of each layer during training, so that they can perform local training independently.\n",
        "\n",
        "    Member variables:\n",
        "    lain_layers (LayerOfLain list): used to store layers for constructing SFF spiking neural network.\n",
        "    insight_pos (float): The key constant for SFF to realize layer collaboration, which is the sum of the goodness of each layer after positive data propagation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lain_dimension):\n",
        "        super().__init__()\n",
        "        self.lain_layers = []\n",
        "        self.insight_pos = 0.\n",
        "        self.insight_neg = 0.\n",
        "        for d in range(len(lain_dimension) - 1):\n",
        "            if(d == 0):\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100.)\n",
        "              self.lain_layers.append(layer)\n",
        "            else:\n",
        "              layer = LayerOfLain(lain_dimension[d], lain_dimension[d + 1], pre_time_au = 2., post_time_au = 100., learning_rate = 0.004, threshold_both=0.04)\n",
        "              self.lain_layers.append(layer)\n",
        "\n",
        "    def network_train_layers(self, train_data_loader, epo):\n",
        "      torch.cuda.empty_cache()\n",
        "      for i, lain_layer in enumerate(self.lain_layers):\n",
        "        print('training layer', i, '...')\n",
        "        for features, labels in tqdm(train_data_loader):\n",
        "          #if(epo > i*1):\n",
        "            #break\n",
        "          torch.cuda.empty_cache()\n",
        "          features, labels = features.to(device), labels.to(device)\n",
        "          features_pos = label_encoder(features, labels)\n",
        "          rnd = torch.randperm(features.size(0))\n",
        "          features_neg = label_encoder(features, labels[rnd])\n",
        "          features_pos = poisson_iter(features_pos, lain_layer.time_step)\n",
        "          features_pos = features_pos.to(device)\n",
        "          features_neg = poisson_iter(features_neg, lain_layer.time_step)\n",
        "          features_neg = features_neg.to(device)\n",
        "          del features, labels\n",
        "          torch.cuda.empty_cache()\n",
        "          #features_pos = features_pos.transpose(0, 1)\n",
        "          #features_neg = features_neg.transpose(0, 1)\n",
        "          self.insight_pos = self.network_collaboration(features_pos)\n",
        "          self.insight_neg = self.network_collaboration(features_neg)\n",
        "          positive_hidden, negative_hidden = features_pos, features_neg\n",
        "          if(i > 0) :\n",
        "            for o in range(i):\n",
        "              positive_hidden, negative_hidden = self.lain_layers[o].forward_withOUT_training(positive_hidden, negative_hidden)\n",
        "              functional.reset_net(self.lain_layers[o].single_net)\n",
        "          torch.cuda.empty_cache()\n",
        "          if(i==0):\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=False)\n",
        "          else:\n",
        "            lain_layer.forward_with_training(positive_hidden, negative_hidden, self.insight_pos, self.insight_neg, stdpflag=False)\n",
        "\n",
        "    def network_predict(self, input):\n",
        "      every_labels_goodness = []\n",
        "      for label in range(10):\n",
        "        hidden = label_encoder(input, label)\n",
        "        hidden = poisson_iter(hidden, 50)\n",
        "        hidden = hidden.to(device)\n",
        "        torch.cuda.empty_cache()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "        every_labels_goodness += [sum(every_layer_goodness).unsqueeze(1)]\n",
        "        del hidden\n",
        "        for lain_layer in self.lain_layers:\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "        torch.cuda.empty_cache()\n",
        "      every_labels_goodness = torch.cat(every_labels_goodness, 1)\n",
        "      return every_labels_goodness.argmax(1)\n",
        "\n",
        "    def network_collaboration(self, input):\n",
        "        hidden = input.clone()\n",
        "        every_layer_goodness = []\n",
        "        for p, lain_layer in enumerate(self.lain_layers):\n",
        "          hidden = lain_layer.forward_withOUT_training_single(hidden, p)\n",
        "          goodnesstem = []\n",
        "          for t in range(lain_layer.time_step):\n",
        "            goodnesstem.append((hidden[t].pow(2).mean(1)).unsqueeze(0))\n",
        "          every_layer_goodness += [(torch.cat(goodnesstem, dim=0)).sum(0)]\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "        del hidden\n",
        "        torch.cuda.empty_cache()\n",
        "        return sum(every_layer_goodness)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(1000)\n",
        "    torch.cuda.empty_cache()\n",
        "    alice = NetOfLain([784, 1000, 400, 200])\n",
        "    for epo in range(1):\n",
        "      print(\"Epoch:\", epo)\n",
        "      torch.cuda.empty_cache()\n",
        "      alice.network_train_layers(train_data_loader, epo)\n",
        "      countT = 0.\n",
        "      lossT = 0.\n",
        "      for test_x, test_y in test_data_loader:\n",
        "        test_x, test_y = test_x.to(device), test_y.to(device)\n",
        "        lossT += 1.0 - alice.network_predict(test_x).eq(test_y).float().mean().item()\n",
        "        countT += 1\n",
        "        for lain_layer in alice.lain_layers:\n",
        "          functional.reset_net(lain_layer.single_net)\n",
        "      print('test error:', lossT / countT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HSqj3MNLkzZ",
        "outputId": "5514f4a0-faf7-47e8-8e0f-5f9fced95ff2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spikingjelly\n",
            "  Downloading spikingjelly-0.0.0.0.14-py3-none-any.whl (437 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/437.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/437.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (2.0.1+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (4.66.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (0.15.2+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->spikingjelly) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->spikingjelly) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->spikingjelly) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->spikingjelly) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->spikingjelly) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->spikingjelly) (1.3.0)\n",
            "Installing collected packages: spikingjelly\n",
            "Successfully installed spikingjelly-0.0.0.0.14\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 117318246.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 15023650.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4422102/4422102 [00:00<00:00, 64644393.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13118029.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Epoch: 0\n",
            "training layer 0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [01:02<00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training layer 1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [01:10<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training layer 2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [01:17<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test error: 0.3813999742269516\n"
          ]
        }
      ]
    }
  ]
}